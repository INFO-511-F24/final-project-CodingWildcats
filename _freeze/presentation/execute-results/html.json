{
  "hash": "a84b5a7e18d11b2ec925b8de833a0fc0",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Stroke Prediction Based on Demographics and Medical History\"\nsubtitle: \"INFO 511 - Fall 2024 - Final Project\"\nauthor: \"Danielle Stea, Erika Kirkpatrick, Kai Shuen Neo, Sahand Motamenim, Rohit Kalakala\"\ntitle-slide-attributes:\n  data-background-image: images/watercolour_sys02_img34_teacup-ocean.jpg\n  data-background-size: stretch\n  data-background-opacity: \"0.7\"\n  data-slide-number: none\nformat:\n  revealjs:\n    theme:  ['data/customtheming.scss']\n  \neditor: visual\nexecute:\n  echo: false\njupyter: python3\n---\n\n\n\n\n# Stroke Prediction with Machine Learning\n\n## Introduction\n\n-   Strokes can be a deadly medical condition, and even if the patient survives there can be life long consequences as a result of the stroke.\n\n-   This is why we wanted to look into a dataset that may allow us to help predict the risk factors that cause a stroke.\n\n## Dataset\n\n-   The dataset used is from a study published in China in 2020. It represents a case group of individuals who had a stroke, and a control group of those who did not.\n\n## Data\n\n-   The dataset included observations like age, gender, marital status, and employment status, as well as medical information such as BMI, glucose levels, smoking history, etc.\n\n::: {#loaddata .cell execution_count=4}\n\n::: {.cell-output .cell-output-stdout}\n```\n      id  gender   age  hypertension  heart_disease ever_married  \\\n0   9046    Male  67.0             0              1          Yes   \n1  51676  Female  61.0             0              0          Yes   \n2  31112    Male  80.0             0              1          Yes   \n3  60182  Female  49.0             0              0          Yes   \n4   1665  Female  79.0             1              0          Yes   \n\n       work_type Residence_type  avg_glucose_level   bmi   smoking_status  \\\n0        Private          Urban             228.69  36.6  formerly smoked   \n1  Self-employed          Rural             202.21   NaN     never smoked   \n2        Private          Rural             105.92  32.5     never smoked   \n3        Private          Urban             171.23  34.4           smokes   \n4  Self-employed          Rural             174.12  24.0     never smoked   \n\n   stroke  \n0       1  \n1       1  \n2       1  \n3       1  \n4       1  \n```\n:::\n:::\n\n\n## Data\n\n-   Qualitative variables: gender, smoking status, employment status, marital status\n-   Quantitative variables: age, BMI, glucose index\n\n::: {#loaddatainfo .cell execution_count=5}\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 5110 entries, 0 to 5109\nData columns (total 12 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   id                 5110 non-null   int64  \n 1   gender             5110 non-null   object \n 2   age                5110 non-null   float64\n 3   hypertension       5110 non-null   int64  \n 4   heart_disease      5110 non-null   int64  \n 5   ever_married       5110 non-null   object \n 6   work_type          5110 non-null   object \n 7   Residence_type     5110 non-null   object \n 8   avg_glucose_level  5110 non-null   float64\n 9   bmi                4909 non-null   float64\n 10  smoking_status     5110 non-null   object \n 11  stroke             5110 non-null   int64  \ndtypes: float64(3), int64(4), object(5)\nmemory usage: 479.2+ KB\nNone\n```\n:::\n:::\n\n\n# **Methods and Results**\n\n## EDA\n\n-   Missing values were dropped from the dataset and categorical variables (gender, marital status, work type, residence type and smoking status) were transformed into factors.\n\n\n\n::: {#plotmissing .cell execution_count=7}\n\n::: {#plotmissing-1 .cell-output .cell-output-display}\n```\n<Figure size 30000x30000 with 0 Axes>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](presentation_files/figure-revealjs/plotmissing-output-2.png){#plotmissing-2 width=2155 height=1506}\n:::\n:::\n\n\n## Variable Correlation Heatmap\n\n-   The correlation matrix of all variables, minus the ID number, is displayed on the heatmap.\n\n![](images/variable_correlation_heatmap.png)\n\n\n\n::: {#cell-plotcorrelation .cell execution_count=9}\n\n::: {.cell-output .cell-output-display}\n![](presentation_files/figure-revealjs/plotcorrelation-output-1.png){#plotcorrelation width=2722 height=2427}\n:::\n:::\n\n\n## **Machine Learning**\n\n-   After preparing the dataset, we split it into training (80%) and testing (20%) sets.\n-   We trained five different machine learning models on this dataset: Random Forest, Decision Tree, Support Vector Machine (SVM), Artificial Neural Network (ANN), and Logistic Regression.\n-   To evaluate the performance of these models, we employed 5-fold cross-validation.\n\n## **Results**\n\n::: {#cell-MLmodels .cell execution_count=10}\n\n::: {.cell-output .cell-output-stdout}\n```\n\nAccuracy for Each Fold and Average Accuracy for Each Model:\n```\n:::\n\n::: {#mlmodels .cell-output .cell-output-display execution_count=10}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>Fold 1</th>\n      <th>Fold 2</th>\n      <th>Fold 3</th>\n      <th>Fold 4</th>\n      <th>Fold 5</th>\n      <th>Average Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Random Forest</td>\n      <td>0.961832</td>\n      <td>0.949109</td>\n      <td>0.964331</td>\n      <td>0.971975</td>\n      <td>0.955414</td>\n      <td>0.960532</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Decision Tree</td>\n      <td>0.924936</td>\n      <td>0.913486</td>\n      <td>0.926115</td>\n      <td>0.927389</td>\n      <td>0.918471</td>\n      <td>0.922079</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Support Vector Machine</td>\n      <td>0.963104</td>\n      <td>0.949109</td>\n      <td>0.964331</td>\n      <td>0.970701</td>\n      <td>0.954140</td>\n      <td>0.960277</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Artificial Neural Network</td>\n      <td>0.963104</td>\n      <td>0.946565</td>\n      <td>0.961783</td>\n      <td>0.970701</td>\n      <td>0.954140</td>\n      <td>0.959259</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Logistic Regression</td>\n      <td>0.963104</td>\n      <td>0.949109</td>\n      <td>0.965605</td>\n      <td>0.970701</td>\n      <td>0.954140</td>\n      <td>0.960532</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## **Analysis of results**\n\n-   Random Forest model achieved the highest average accuracy at 96.05%, which was matched by Logistic Regression. The Support Vector Machine and Artificial Neural Network followed closely, with average accuracies of 96.03% and 95.93%, respectively. The Decision Tree model had the lowest performance, with an average accuracy of 92.21%.\n-   In summary, the use of cross-validation allowed us to obtain a more accurate and unbiased assessment of model performance.\n-   Among the models, Random Forest and Logistic Regression showed the best overall performance in terms of both average accuracy and stability. Support Vector Machine and Artificial Neural Network also performed well but did not outperform Random Forest or Logistic Regression. The Decision Tree model had the lowest accuracy and was less stable, making it a less desirable choice for this problem. If model simplicity, speed, and interpretability are essential, Logistic Regression is an excellent option. However, if model robustness and predictive power are the priority, Random Forest would be the better choice.\n\n# **Conclusions**\n\n## Conclusion\n\n-   There was not a particularly strong correlation between the stroke outcome and the covariates listed in the dataset. The strongest correlation to a stroke outcome was with age at 0.23. Other notable correlations to a stroke outcome were hypertension, heart disease and glucose level, all at 0.14. It appears that the strongest correlations to a stroke outcome occur with participantsâ€™ physical properties rather than their demographics or living status.\n-   The model results show that individuals in this dataset could be classified by their stroke outcome based on the covariates age, gender, marital status, and employment status, BMI, glucose levels, and smoking history with a high degree of accuracy. Since there did not appear to be a particularly strong correlation to any one covariate, it was likely a combination of these factors associated with a stroke outcome.\n\n## Future work\n\n-   One of the limitations we encountered with this dataset is that there was actually a very small proportion of people who had a stroke compared to those that did not: 4.87% (N=249) of individuals had a stroke, while 95.1% (N=4861) did not. This created an unequal distribution within both the training and testing dataset, which may have artificially boosted the classification accuracy. Future work could be to increase the population of people being surveyed.\n-   The context upon which the study was conducted was not able to return insights that we could consider to be generalizable across the general public internationally. The dataset was from a study that was conducted in China, with its funding agency being the National Natural Science Foundation of China and ADAPT Centre for Digital Content Technology funded through the SFI Research Centres Programme. The population studied was mainly the Chinese population. As such, observations gathered from the study would be considered more applicable towards the Asian population instead of the international population.Â Future work could be to increase the diversity of the population of people being surveyed.\n\n## **References**\n\n-   2020: Pathan, Muhammad Salman & Zhang, Jianbiao & John, Deepu & Nag, Avishek & Dev, Soumyabrata. (2020). Identifying Stroke Indicators Using Rough Sets. IEEE Access. 8. 10.1109/ACCESS.2020.3039439\n\n",
    "supporting": [
      "presentation_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}