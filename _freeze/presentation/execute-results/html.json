{
  "hash": "fdbc773a6155aca1398ab615b8551312",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Stroke Prediction Based on Demographics and Medical History\"\nsubtitle: \"INFO 511 - Fall 2024 - Final Project\"\nauthor: \"Danielle Stea, Erika Kirkpatrick, Kai Shuen Neo, Sahand Motamenim, Rohit Kalakala\"\ntitle-slide-attributes:\n  data-background-image: images/watercolour_sys02_img34_teacup-ocean.jpg\n  data-background-size: stretch\n  data-background-opacity: \"0.7\"\n  data-slide-number: none\nformat:\n  revealjs:\n    theme:  ['data/customtheming.scss']\n  \neditor: visual\nexecute:\n  echo: false\njupyter: python3\n---\n\n\n\n\n# Stroke Prediction with Machine Learning\n\n## Introduction\n\n-   Strokes can be a deadly medical condition, and even if the patient survives there can be life long consequences as a result of the stroke.\n\n-   This is why we wanted to look into a dataset that may allow us to help predict the risk factors that cause a stroke.\n\n## Dataset\n\n-   The dataset used is from a study published in China in 2020. It represents a case group of individuals who had a stroke, and a control group of those who did not.\n\n## Data\n\n-   The dataset included observations like age, gender, marital status, and employment status, as well as medical information such as BMI, glucose levels, smoking history, etc.\n\n::: {#loaddata .cell execution_count=4}\n\n::: {.cell-output .cell-output-stdout}\n```\n      id  gender   age  hypertension  heart_disease ever_married  \\\n0   9046    Male  67.0             0              1          Yes   \n1  51676  Female  61.0             0              0          Yes   \n2  31112    Male  80.0             0              1          Yes   \n3  60182  Female  49.0             0              0          Yes   \n4   1665  Female  79.0             1              0          Yes   \n\n       work_type Residence_type  avg_glucose_level   bmi   smoking_status  \\\n0        Private          Urban             228.69  36.6  formerly smoked   \n1  Self-employed          Rural             202.21   NaN     never smoked   \n2        Private          Rural             105.92  32.5     never smoked   \n3        Private          Urban             171.23  34.4           smokes   \n4  Self-employed          Rural             174.12  24.0     never smoked   \n\n   stroke  \n0       1  \n1       1  \n2       1  \n3       1  \n4       1  \n```\n:::\n:::\n\n\n## Data\n\n-   Qualitative variables: gender, smoking status, employment status, marital status\n-   Quantitative variables: age, BMI, glucose index\n\n::: {#loaddatainfo .cell execution_count=5}\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 5110 entries, 0 to 5109\nData columns (total 12 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   id                 5110 non-null   int64  \n 1   gender             5110 non-null   object \n 2   age                5110 non-null   float64\n 3   hypertension       5110 non-null   int64  \n 4   heart_disease      5110 non-null   int64  \n 5   ever_married       5110 non-null   object \n 6   work_type          5110 non-null   object \n 7   Residence_type     5110 non-null   object \n 8   avg_glucose_level  5110 non-null   float64\n 9   bmi                4909 non-null   float64\n 10  smoking_status     5110 non-null   object \n 11  stroke             5110 non-null   int64  \ndtypes: float64(3), int64(4), object(5)\nmemory usage: 479.2+ KB\nNone\n```\n:::\n:::\n\n\n# **Methods and Results**\n\n## EDA\n\n-   Missing values were dropped from the dataset and categorical variables (gender, marital status, work type, residence type and smoking status) were transformed into factors.\n\n\n\n::: {#plotmissing .cell execution_count=7}\n\n::: {#plotmissing-1 .cell-output .cell-output-display}\n```\n<Figure size 30000x30000 with 0 Axes>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](presentation_files/figure-revealjs/plotmissing-output-2.png){#plotmissing-2 width=2155 height=1506}\n:::\n:::\n\n\n## Variable Correlation Heatmap\n\n-   The correlation matrix of all variables, minus the ID number, is displayed on the heatmap.\n\n\n\n::: {#cell-plotcorrelation .cell execution_count=9}\n\n::: {.cell-output .cell-output-display}\n![](presentation_files/figure-revealjs/plotcorrelation-output-1.png){#plotcorrelation width=2722 height=2427}\n:::\n:::\n\n\n## **Machine Learning**\n\n-   After preparing the dataset, we split it into training (80%) and testing (20%) sets.\n-   We trained five different machine learning models on this dataset: Random Forest, Decision Tree, Support Vector Machine (SVM), Artificial Neural Network (ANN), and Logistic Regression.\n-   To evaluate the performance of these models, we employed 5-fold cross-validation.\n\n## **Results**\n\n::: {#cell-MLmodels .cell execution_count=10}\n\n::: {.cell-output .cell-output-stdout}\n```\n\nAccuracy for Each Fold and Average Accuracy for Each Model:\n```\n:::\n\n::: {#mlmodels .cell-output .cell-output-display execution_count=10}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>Fold 1</th>\n      <th>Fold 2</th>\n      <th>Fold 3</th>\n      <th>Fold 4</th>\n      <th>Fold 5</th>\n      <th>Average Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Random Forest</td>\n      <td>0.961832</td>\n      <td>0.949109</td>\n      <td>0.964331</td>\n      <td>0.971975</td>\n      <td>0.955414</td>\n      <td>0.960532</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Decision Tree</td>\n      <td>0.924936</td>\n      <td>0.913486</td>\n      <td>0.926115</td>\n      <td>0.927389</td>\n      <td>0.918471</td>\n      <td>0.922079</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Support Vector Machine</td>\n      <td>0.963104</td>\n      <td>0.949109</td>\n      <td>0.964331</td>\n      <td>0.970701</td>\n      <td>0.954140</td>\n      <td>0.960277</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Artificial Neural Network</td>\n      <td>0.963104</td>\n      <td>0.946565</td>\n      <td>0.961783</td>\n      <td>0.970701</td>\n      <td>0.954140</td>\n      <td>0.959259</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Logistic Regression</td>\n      <td>0.963104</td>\n      <td>0.949109</td>\n      <td>0.965605</td>\n      <td>0.970701</td>\n      <td>0.954140</td>\n      <td>0.960532</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n# **Conclusions**\n\n## Conclusion and Future work\n\n## **References**\n\n-   2020: Pathan, Muhammad Salman & Zhang, Jianbiao & John, Deepu & Nag, Avishek & Dev, Soumyabrata. (2020). Identifying Stroke Indicators Using Rough Sets. IEEE Access. 8. 10.1109/ACCESS.2020.3039439\n\n",
    "supporting": [
      "presentation_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}