{
  "hash": "282020f590a1ebda02eec3142043111d",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"College Characteristics and Student Success\"\nsubtitle: \"INFO 511 - Fall 2024 - Final Project\"\nauthor: \n  - name: \"Coding Wildcats\"\n    affiliations:\n      - name: \"School of Information, University of Arizona\"\nformat:\n   html:\n    code-tools: true\n    code-overflow: wrap\n    embed-resources: true\neditor: visual\nexecute:\n  warning: false\n  echo: false\njupyter: python3\n---\n\n\n\n\n## Abstract\n\nIn this project, we aimed to predict whether a person is at risk of having a stroke using machine learning models. We used a healthcare dataset containing information about individuals (like gender, age, health status, etc.). Our goal was to train multiple machine learning models and evaluate their classification accuracy.\n\n## Introduction\n\nAdd introduction here.\n\n## Data\n\n\n\nSource: https://www.kaggle.com/datasets/fedesoriano/stroke-prediction-dataset\n\nThis data comes from a study published in China in 2020: Pathan, Muhammad Salman & Zhang, Jianbiao & John, Deepu & Nag, Avishek & Dev, Soumyabrata. (2020). Identifying Stroke Indicators Using Rough Sets. IEEE Access. 8. 10.1109/ACCESS.2020.3039439.\\\\\n\n## Methodology\n\nThe first step of our data cleaning is to evaluate how much of our data is missing. Of 12 variables, only BMI has missing values.\n\n\n\n::: {#plotmissing .cell execution_count=3}\n\n::: {#plotmissing-1 .cell-output .cell-output-display}\n```\n<Figure size 960x576 with 0 Axes>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/plotmissing-output-2.png){#plotmissing-2 width=679 height=490}\n:::\n:::\n\n\n\n\nMissing values were dropped from the dataset and categorical variables (gender, marital status, work type, residence type and smoking status) were transformed into factors. The correlation matrix of all variables, minus the ID number, is displayed on the heatmap:\n\n::: {#cell-plotcorrelation .cell execution_count=5}\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/plotcorrelation-output-1.png){#plotcorrelation width=853 height=760}\n:::\n:::\n\n\nThe data was split into training (70% of total data) and testing (30% of total data) datasets. A logistic regression model was fit to the training dataset. The model accuracy was then evaluated by comparing its predictions against the testing data.\n\n::: {#cell-logisticregression .cell execution_count=6}\n\n::: {.cell-output .cell-output-stdout}\n```\nThe accuracy of the logistic regression model is  96.33401221995926 %\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/logisticregression-output-2.png){#logisticregression width=568 height=482}\n:::\n:::\n\n\nAfter preparing the dataset, we split it into training and testing sets. The training set, which comprises 80% of the data, is used to train our machine learning models, while the testing set, representing 20% of the data, is reserved for final evaluation. This approach ensures that we have unseen data to measure how well the models generalize.\n\nWe trained five different machine learning models on this dataset: Random Forest, Decision Tree, Support Vector Machine (SVM), Artificial Neural Network (ANN), and Logistic Regression. Each of these models uses a unique method to identify patterns in the data. The Random Forest model, for instance, builds multiple decision trees and combines their predictions to create a more stable and robust model. The Decision Tree model uses a single tree to make predictions, which is often more prone to overfitting. The Support Vector Machine works by finding the optimal hyperplane that separates classes in the feature space. The Artificial Neural Network mimics how neurons in the human brain process information, making it effective for complex relationships in the data. Finally, Logistic Regression is a linear model that is simple, interpretable, and computationally efficient.\n\nTo evaluate the performance of these models, we employed 5-fold cross-validation. This method splits the training data into five equally sized segments (folds). Each model is trained on four of these folds while the fifth fold is used as a validation set. This process is repeated five times, with each fold serving as a validation set once. Cross-validation provides a more reliable estimate of the model's performance because it reduces the likelihood of overfitting to a single train-test split. For each model, we calculated the accuracy for each fold, as well as the average accuracy across all five folds.\n\nThe results indicate that the Random Forest model achieved the highest average accuracy at 96.05%, which was matched by Logistic Regression. The Support Vector Machine and Artificial Neural Network followed closely, with average accuracies of 96.03% and 95.93%, respectively. The Decision Tree model had the lowest performance, with an average accuracy of 92.21%. This is not surprising since Decision Trees are known to be sensitive to small changes in the training data, leading to higher variance. On the other hand, Random Forest, which combines multiple Decision Trees, reduces this variance and produces more stable predictions.\n\nThe consistency of model performance across the five folds highlights the robustness of the Random Forest, Logistic Regression, and SVM models. Their accuracies for each fold were relatively stable, with small deviations from the average, indicating that these models generalize well to new, unseen data. The Decision Tree model exhibited slightly higher variability across the folds, further supporting the observation that it is less stable than the other models.\n\nIn summary, the use of cross-validation allowed us to obtain a more accurate and unbiased assessment of model performance. Among the models, Random Forest and Logistic Regression showed the best overall performance in terms of both average accuracy and stability. Support Vector Machine and Artificial Neural Network also performed well but did not outperform Random Forest or Logistic Regression. The Decision Tree model had the lowest accuracy and was less stable, making it a less desirable choice for this problem. If model simplicity, speed, and interpretability are essential, Logistic Regression is an excellent option. However, if model robustness and predictive power are the priority, Random Forest would be the better choice.\n\n::: {#289ed9fd .cell execution_count=7}\n\n::: {.cell-output .cell-output-stdout}\n```\n\nAccuracy for Each Fold and Average Accuracy for Each Model:\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=7}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>Fold 1</th>\n      <th>Fold 2</th>\n      <th>Fold 3</th>\n      <th>Fold 4</th>\n      <th>Fold 5</th>\n      <th>Average Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Random Forest</td>\n      <td>0.961832</td>\n      <td>0.949109</td>\n      <td>0.964331</td>\n      <td>0.971975</td>\n      <td>0.955414</td>\n      <td>0.960532</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Decision Tree</td>\n      <td>0.924936</td>\n      <td>0.913486</td>\n      <td>0.926115</td>\n      <td>0.927389</td>\n      <td>0.918471</td>\n      <td>0.922079</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Support Vector Machine</td>\n      <td>0.963104</td>\n      <td>0.949109</td>\n      <td>0.964331</td>\n      <td>0.970701</td>\n      <td>0.954140</td>\n      <td>0.960277</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Artificial Neural Network</td>\n      <td>0.963104</td>\n      <td>0.946565</td>\n      <td>0.961783</td>\n      <td>0.970701</td>\n      <td>0.954140</td>\n      <td>0.959259</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Logistic Regression</td>\n      <td>0.963104</td>\n      <td>0.949109</td>\n      <td>0.965605</td>\n      <td>0.970701</td>\n      <td>0.954140</td>\n      <td>0.960532</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## Results\n\n## Discussion\n\nAdd discussion here.\n\n## References\n\nAdd references here.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}