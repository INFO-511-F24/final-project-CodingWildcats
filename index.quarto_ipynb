{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Stroke Prediction Based on Demographics and Medical History\"\n",
        "subtitle: \"INFO 511 - Fall 2024 - Final Project\"\n",
        "author: \n",
        "  - name: \"Coding Wildcats\"\n",
        "    affiliations:\n",
        "      - name: \"School of Information, University of Arizona\"\n",
        "format:\n",
        "   html:\n",
        "    code-tools: true\n",
        "    code-overflow: wrap\n",
        "    embed-resources: true\n",
        "editor: visual\n",
        "execute:\n",
        "  warning: false\n",
        "  echo: false\n",
        "jupyter: python3\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Abstract\n",
        "\n",
        "In this project, we aimed to predict whether a person is at risk of having a stroke using machine learning models. We used a healthcare dataset containing information about individuals (like gender, age, health status, etc.). Our goal was to train multiple machine learning models and evaluate their classification accuracy.\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Taking care of oneâ€™s health is an important consideration for anyone, but especially young adults who are just starting their own lives. For our project we wanted to do our analysis on data relating to strokes and the risk of strokes. Our models attempt to predict whether a number of factors can be correlated with the risk of strokes. It is important to understand the risk factors of strokes (and other illnesses) because it is much easier to change your lifestyle young but also be aware that family members may be at a high risk of having a stroke and be prepared by knowing the signs of a stroke. This project could help many people, young and old, so they know the risk factors that may contribute to the chance of them having a stroke.\n",
        "\n",
        "## Data\n",
        "\n",
        "The data set we have decided to use is from a study published in China in 2020: Pathan, Muhammad Salman & Zhang, Jianbiao & John, Deepu & Nag, Avishek & Dev, Soumyabrata. (2020). Identifying Stroke Indicators Using Rough Sets. IEEE Access. 8. 10.1109/ACCESS.2020.3039439. The data from this study was then taken and uploaded on Kaggle, which is where we first found it, though we also went directly to the source to determine validity. Since this data set does deal with patient information there are some ethical considerations, but the study that published this data has de-identified it, removing personal identifiers so the chances of being able to determine the identity of any one individual in this study from this data alone is quite small. The data set has 12 columns, with observations including demographic information such as age, gender, marital status, and employment status, as well as medical information such as BMI, glucose levels, smoking history, etc. A glimpse at the dataset can be found below:\n"
      ],
      "id": "af3e73e3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: loaddata\n",
        "#| echo: false\n",
        "import pandas as pd\n",
        "stroke = pd.read_csv(\"data/healthcare-dataset-stroke-data.csv\")\n",
        "print(stroke.head())"
      ],
      "id": "loaddata",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The dataset includes variables such as gender, smoking status, employment status, and marital status which are qualitative variables. Other variables such as age, BMI, and glucose index are all quantitative.\n"
      ],
      "id": "704a5f0a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: loaddatainfo\n",
        "#| echo: false\n",
        "import pandas as pd\n",
        "stroke = pd.read_csv(\"data/healthcare-dataset-stroke-data.csv\")\n",
        "print(stroke.info())"
      ],
      "id": "loaddatainfo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As mentioned this dataset has several demographic variables and then a variable that tracks if that patient had a stroke. From this, we hope to be able to create a model to predict the possibility of an individual having a stroke based on their demographic information and medical history of an individual. Which is important because identifying key risk factors of a stroke can be used for prevention and more effective monitoring of patients.\n",
        "\n",
        "Source: https://www.kaggle.com/datasets/fedesoriano/stroke-prediction-dataset\n",
        "\n",
        "This data comes from a study published in China in 2020: Pathan, Muhammad Salman & Zhang, Jianbiao & John, Deepu & Nag, Avishek & Dev, Soumyabrata. (2020). Identifying Stroke Indicators Using Rough Sets. IEEE Access. 8. 10.1109/ACCESS.2020.3039439.\n",
        "\n",
        "## Visualizations\n",
        "\n",
        "Across the population, there is a higher proportion of people who do not suffer from stroke as compared to those who do suffer from stroke.\n"
      ],
      "id": "d46560cb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: visualization_1\n",
        "#| echo: false\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "plt.figure(figsize=(5,3))\n",
        "ax = sns.countplot(x='stroke', data=stroke)\n",
        "plt.title('Distribution of stroke')"
      ],
      "id": "visualization_1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As for gender analysis, there is a higher proportion of females as compared to males surveyed.\n"
      ],
      "id": "9ce3186a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: visualization_2\n",
        "#| echo: false\n",
        "plt.figure(figsize = (6, 6))\n",
        "count_female = (stroke['gender'] == 'Female').sum()\n",
        "count_male = (stroke['gender'] == 'Male').sum()\n",
        "temp = [count_female,count_male]\n",
        "labels = ['Female','Male']\n",
        "textprops = {\"fontsize\":12}\n",
        "plt.pie(temp,labels=labels,autopct='%1.2f%%',textprops=textprops)\n",
        "plt.title('Gender Distribution')\n",
        "plt.show()"
      ],
      "id": "visualization_2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In terms of residence type, the distribution for rural vs urban are highly equivalent.\n"
      ],
      "id": "4dd2465f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: visualization_3\n",
        "#| echo: false\n",
        "plt.figure(figsize = (6, 6))\n",
        "count_urban = (stroke['Residence_type'] == 'Urban').sum()\n",
        "count_rural = (stroke['Residence_type'] == 'Rural').sum()\n",
        "temp = [count_urban,count_rural]\n",
        "labels = ['Urban','Rural']\n",
        "textprops = {\"fontsize\":12}\n",
        "plt.pie(temp,labels=labels,autopct='%1.2f%%', textprops=textprops)\n",
        "plt.title('Distribution for Residence Type')\n",
        "plt.show()"
      ],
      "id": "visualization_3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As for marital status, there is a higher proportion of people surveyed who are married, as compared to those who are not married.\n"
      ],
      "id": "081de146"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: visualization_4\n",
        "#| echo: false\n",
        "plt.figure(figsize = (6, 6))\n",
        "count_yes = (stroke['ever_married'] == 'Yes').sum()\n",
        "count_no = (stroke['ever_married'] == 'No').sum()\n",
        "temp = [count_yes,count_no]\n",
        "labels = ['Married','Not Married']\n",
        "textprops = {\"fontsize\":12}\n",
        "plt.pie(temp,labels=labels,autopct='%1.2f%%',textprops=textprops)\n",
        "plt.title('Distribution for Marital Status')\n",
        "plt.show()"
      ],
      "id": "visualization_4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As for the distribution for work type across the population, a majority of the people surveyed worked in private, with lesser distribution of the population working as self-employed, looking after children, or working government jobs. There is a small proportion of the population who have never worked.\n"
      ],
      "id": "9ceb2726"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: visualization_5\n",
        "#| echo: false\n",
        "plt.figure(figsize=(6,6))\n",
        "count_private = (stroke['work_type'] == 'Private').sum()\n",
        "count_self_employed = (stroke['work_type'] == 'Self-employed').sum()\n",
        "count_children = (stroke['work_type'] == 'children').sum()\n",
        "count_govt_job = (stroke['work_type'] == 'Govt_job').sum()\n",
        "count_never_worked = (stroke['work_type'] == 'Never_worked').sum()\n",
        "temp = [count_private,count_self_employed, count_children, count_govt_job, count_never_worked]\n",
        "textprops = {\"fontsize\":12}\n",
        "plt.pie(temp,labels=['Private','Self employed', 'Children','Govt Job','Never Worked'], autopct='%1.2f%%',textprops=textprops)\n",
        "plt.title('Distribution for Work Type')\n",
        "plt.show()"
      ],
      "id": "visualization_5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "While for the distribution of smoking status, there is a higher proportion of the population who have never smoked. Followed by a smaller proportion of the population with smoking status being unknown and formerly smoked. Only about 15% of the population surveyed smokes.\n"
      ],
      "id": "ffc7c07f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: visualization_6\n",
        "#| echo: false\n",
        "plt.figure(figsize = (6, 6))\n",
        "count_never_smoked = (stroke['smoking_status'] == 'never smoked').sum()\n",
        "count_formerly_smoked = (stroke['smoking_status'] == 'formerly smoked').sum()\n",
        "count_smokes = (stroke['smoking_status'] == 'smokes').sum()\n",
        "count_unknown = (stroke['smoking_status'] == 'Unknown').sum()\n",
        "temp = [count_never_smoked,count_formerly_smoked,count_smokes,count_unknown]\n",
        "labels = ['Never smoked','Formerly smoked', 'Smokes','Unknown']\n",
        "textprops = {\"fontsize\":12}\n",
        "colors = ['red', '#orange', '#green', '#blue']\n",
        "plt.pie(temp,labels=labels,autopct='%1.2f%%',textprops=textprops, colors=colors)\n",
        "plt.title('Distribution for smoking status')\n",
        "plt.show()"
      ],
      "id": "visualization_6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Methodology\n",
        "\n",
        "The first step of our data cleaning is to evaluate how much of our data is missing. Of 12 variables, only BMI has missing values.\n"
      ],
      "id": "24917202"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: importpackages\n",
        "#| echo: false\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "id": "importpackages",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: plotmissing\n",
        "#| echo: false\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.displot(\n",
        "    data=stroke.isna().melt(value_name=\"missing\"),\n",
        "    y=\"variable\",\n",
        "    hue=\"missing\",\n",
        "    multiple=\"fill\",\n",
        "    aspect=1.25\n",
        ").set(title='Proportion of Missing Values in Dataset')"
      ],
      "id": "plotmissing",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: cleandata\n",
        "#| echo: false\n",
        "stroke = stroke.dropna()\n",
        "stroke['gender'] = LabelEncoder().fit_transform(stroke['gender'])\n",
        "stroke['ever_married'] = LabelEncoder().fit_transform(stroke['ever_married'])\n",
        "stroke['work_type'] = LabelEncoder().fit_transform(stroke['work_type'])\n",
        "stroke['Residence_type'] = LabelEncoder().fit_transform(stroke['Residence_type'])\n",
        "stroke['smoking_status'] = LabelEncoder().fit_transform(stroke['smoking_status'])\n",
        "stroke.drop(columns=['id'],axis=1,inplace=True) "
      ],
      "id": "cleandata",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Missing values were dropped from the dataset and categorical variables (gender, marital status, work type, residence type and smoking status) were transformed into factors. The correlation matrix of all variables, minus the ID number, is displayed on the heatmap:\n"
      ],
      "id": "31ae732e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: plotcorrelation\n",
        "#| echo: false\n",
        "corr = stroke.corr()\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(corr, annot=True)\n",
        "plt.title('Variable Correlation Heatmap')\n",
        "plt.show()"
      ],
      "id": "plotcorrelation",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After preparing the dataset, we split it into training and testing sets. The training set, which comprises 80% of the data, is used to train our machine learning models, while the testing set, representing 20% of the data, is reserved for final evaluation. This approach ensures that we have unseen data to measure how well the models generalize.\n",
        "\n",
        "We trained five different machine learning models on this dataset: Random Forest, Decision Tree, Support Vector Machine (SVM), Artificial Neural Network (ANN), and Logistic Regression. Each of these models uses a unique method to identify patterns in the data. The Random Forest model, for instance, builds multiple decision trees and combines their predictions to create a more stable and robust model. The Decision Tree model uses a single tree to make predictions, which is often more prone to overfitting. The Support Vector Machine works by finding the optimal hyperplane that separates classes in the feature space. The Artificial Neural Network mimics how neurons in the human brain process information, making it effective for complex relationships in the data. Finally, Logistic Regression is a linear model that is simple, interpretable, and computationally efficient.\n",
        "\n",
        "To evaluate the performance of these models, we employed 5-fold cross-validation. This method splits the training data into five equally sized segments (folds). Each model is trained on four of these folds while the fifth fold is used as a validation set. This process is repeated five times, with each fold serving as a validation set once. Cross-validation provides a more reliable estimate of the model's performance because it reduces the likelihood of overfitting to a single train-test split. For each model, we calculated the accuracy for each fold, as well as the average accuracy across all five folds.\n"
      ],
      "id": "9c7dde7c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: MLmodels\n",
        "#| warning: false\n",
        "#| echo: false\n",
        "# Split the data (20/80)\n",
        "X = stroke.drop(columns=['stroke'], axis=1) \n",
        "Y = stroke['stroke']\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the ML models\n",
        "models = {\n",
        "    'Random Forest': RandomForestClassifier(random_state=42),\n",
        "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
        "    'Support Vector Machine': SVC(kernel='linear', random_state=42),\n",
        "    'Artificial Neural Network': MLPClassifier(random_state=42, max_iter=500),\n",
        "    'Logistic Regression': LogisticRegression(max_iter=500, random_state=42)\n",
        "}\n",
        "\n",
        "# Set up cross-validation\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Store the results\n",
        "results = {'Model': [], 'Fold 1': [], 'Fold 2': [], 'Fold 3': [], 'Fold 4': [], 'Fold 5': [], 'Average Accuracy': []}\n",
        "\n",
        "# Train each model, calculate cross-validation accuracy\n",
        "for model_name, model in models.items():\n",
        "    accuracy_scores = cross_val_score(model, X_train, Y_train, cv=kfold, scoring='accuracy')\n",
        "    avg_accuracy = np.mean(accuracy_scores)\n",
        "    # Add results to the results dictionary\n",
        "    results['Model'].append(model_name)\n",
        "    results['Fold 1'].append(accuracy_scores[0])\n",
        "    results['Fold 2'].append(accuracy_scores[1])\n",
        "    results['Fold 3'].append(accuracy_scores[2])\n",
        "    results['Fold 4'].append(accuracy_scores[3])\n",
        "    results['Fold 5'].append(accuracy_scores[4])\n",
        "    results['Average Accuracy'].append(avg_accuracy)\n",
        "\n",
        "# Display the accuracy results in a DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Print the results DataFrame\n",
        "print(\"\\nAccuracy for Each Fold and Average Accuracy for Each Model:\")\n",
        "results_df"
      ],
      "id": "MLmodels",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results\n",
        "\n",
        "The results indicate that the Random Forest model achieved the highest average accuracy at 96.05%, which was matched by Logistic Regression. The Support Vector Machine and Artificial Neural Network followed closely, with average accuracies of 96.03% and 95.93%, respectively. The Decision Tree model had the lowest performance, with an average accuracy of 92.21%. This is not surprising since Decision Trees are known to be sensitive to small changes in the training data, leading to higher variance. On the other hand, Random Forest, which combines multiple Decision Trees, reduces this variance and produces more stable predictions.\n",
        "\n",
        "The consistency of model performance across the five folds highlights the robustness of the Random Forest, Logistic Regression, and SVM models. Their accuracies for each fold were relatively stable, with small deviations from the average, indicating that these models generalize well to new, unseen data. The Decision Tree model exhibited slightly higher variability across the folds, further supporting the observation that it is less stable than the other models.\n",
        "\n",
        "In summary, the use of cross-validation allowed us to obtain a more accurate and unbiased assessment of model performance. Among the models, Random Forest and Logistic Regression showed the best overall performance in terms of both average accuracy and stability. Support Vector Machine and Artificial Neural Network also performed well but did not outperform Random Forest or Logistic Regression. The Decision Tree model had the lowest accuracy and was less stable, making it a less desirable choice for this problem. If model simplicity, speed, and interpretability are essential, Logistic Regression is an excellent option. However, if model robustness and predictive power are the priority, Random Forest would be the better choice.\n",
        "\n",
        "## Discussion\n",
        "\n",
        "We noted in this analysis that there was not a particularly strong correlation between the stroke outcome and the covariates listed in the dataset. The strongest correlation to a stroke outcome was with age at 0.23. Other notable correlations to a stroke outcome were hypertension, heart disease and glucose level, all at 0.14. It appears that the strongest correlations to a stroke outcome occur with participantsâ€™ physical properties rather than their demographics or living status.\n",
        "\n",
        "The model results show that individuals in this dataset could be classified by their stroke outcome based on the covariates age, gender, marital status, and employment status, BMI, glucose levels, and smoking history with a high degree of accuracy. Since there did not appear to be a particularly strong correlation to any one covariate, it was likely a combination of these factors associated with a stroke outcome.\n",
        "\n",
        "One of the limitations we encountered with this dataset is that there was actually a very small proportion of people who had a stroke compared to those that did not: 4.87% (N=249) of individuals had a stroke, while 95.1% (N=4861) did not. This created an unequal distribution within both the training and testing dataset, which may have artificially boosted the classification accuracy.\n",
        "\n",
        "In addition, although high average accuracy was obtained across the 5 models that we have analyzed upon, the context upon which the study was conducted was not able to return insights that we could consider to be generalizable across the general public internationally. The dataset was from a study that was conducted in China, with its funding agency being the National Natural Science Foundation of China and ADAPT Centre for Digital Content Technology funded through the SFI Research Centres Programme. The population studied was mainly the Chinese population. As such, observations gathered from the study would be considered more applicable towards the Asian population instead of the international population.Â \n",
        "\n",
        "## References\n",
        "\n",
        "2020: Pathan, Muhammad Salman & Zhang, Jianbiao & John, Deepu & Nag, Avishek & Dev, Soumyabrata. (2020). Identifying Stroke Indicators Using Rough Sets. IEEE Access. 8. 10.1109/ACCESS.2020.3039439."
      ],
      "id": "69cdb6c9"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/kaishuenneo/.pyenv/versions/3.8.1/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}